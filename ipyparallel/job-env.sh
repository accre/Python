#!/bin/bash

# This file either loads or creates an appropriate conda environment 

NARGS="$#"

case $NARGS in
  (0)
    # Sets a default value for the environment name if not present
    MY_NAME="ipyparallel_env"
    ;;
  (1)
    MY_NAME="$1"
    ;;
  (*)
    (>&2 echo "Error: did not expect more than one argument.")
    (>&2 echo "    (Got $@)")
    exit 1
    ;;
esac

# Loads necessary ACCRE packages
setpkgs -a anaconda3

# Checks if environment name is valid and, if so, exports name
if [[ "$MY_NAME" =~ ^[0-9A-Za-z_]+$ ]]; then
  export MY_CONDA_ENV=$MY_NAME ;
else
  (>&2 echo "Invalid name $MY_NAME$")
  exit 1
fi

# If the conda environment exists, then activates the environment
# else creates the new conda environment with the Makefile
if $(conda env list | grep -q $MY_CONDA_ENV); then
  echo "Found existing conda environment $MY_CONDA_ENV" 
  source activate $MY_CONDA_ENV 
else
  echo "Creating conda environment $MY_CONDA_ENV";
  make env
  source activate $MY_CONDA_ENV 
  make install
  make test
fi

## Set additional environment variables

# PROFILE should point to a network drive, otherwise, the JSON created
# by ipcontroller needs to be copies to each host
export PROFILE=/scratch/$USER/job_${SLURM_JOB_ID}_$(hostname)
echo "Creating profile ${PROFILE}"
ipython profile create ${PROFILE}

# Creates roles for each task to be run in parallel
LAST_PROC=$(( $SLURM_NTASKS - 1 )) # id of the last process
cluster_conf=$"# This file has been generated by $0\n"
cluster_conf+=$"0         ./cluster-roles.sh controller\n"
cluster_conf+=$"1-$LAST_PROC      ./cluster-roles.sh engine"

echo -e $cluster_conf > cluster.conf

# Creates output filename including timestamp
OUTFILE=pi_estimate$(date +%Y%m%d_%H%M%S).txt
echo Using output file $OUTFILE

export APP="python compute_pi.py --profile ${PROFILE} -n 1e12 -o $OUTFILE" 
